#!/bin/bash
# ============================================================
# RUN SMOLLM2-135M EMOTION CLASSIFIER - Ultra Fast Tiny Model
# Target: GPU 0 (~12GB free VRAM - nh∆∞ng model n√†y ch·ªâ c·∫ßn ~300MB!)
# D·ª± ki·∫øn latency: < 10ms (nhanh h∆°n Phi-3 g·∫•p 3-5 l·∫ßn)
# ============================================================

set -e

# Colors
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

echo -e "${GREEN}============================================${NC}"
echo -e "${GREEN}  SMOLLM2-135M EMOTION CLASSIFIER (ULTRA)   ${NC}"
echo -e "${GREEN}============================================${NC}"

# Configuration
VENV_DIR="$HOME/venv_phi3"  # T√°i s·ª≠ d·ª•ng venv c≈©
GPU_ID=0
PORT=30030
MODEL_NAME="HuggingFaceTB/SmolLM2-135M-Instruct"  # ‚ö° TINY MODEL
GPU_MEMORY_UTIL=0.3  # Ch·ªâ c·∫ßn 15% VRAM (~1.8GB)
MAX_MODEL_LEN=512
MAX_NUM_SEQS=512      # TƒÉng batch size v√¨ model nh·ªè


# Step 1: Check GPU
echo -e "\n${YELLOW}[1/3] Checking GPU ${GPU_ID} availability...${NC}"
FREE_MEM=$(nvidia-smi -i $GPU_ID --query-gpu=memory.free --format=csv,noheader,nounits)
echo "GPU ${GPU_ID} Free Memory: ${FREE_MEM} MiB"

if [ "$FREE_MEM" -lt 2000 ]; then
    echo -e "${RED}WARNING: GPU ${GPU_ID} has less than 2GB free!${NC}"
    echo "SmolLM2-135M only needs ~300MB, but 2GB is recommended for overhead."
fi


# Step 2: Activate venv
echo -e "\n${YELLOW}[2/3] Activating Python environment...${NC}"

if [ ! -d "$VENV_DIR" ]; then
    echo -e "${RED}ERROR: Virtual environment not found at $VENV_DIR${NC}"
    echo "Creating new venv..."
    python3.11 -m venv "$VENV_DIR"
    source "$VENV_DIR/bin/activate"
    
    echo "Installing vLLM..."
    pip install --upgrade pip
    pip install vllm==0.12.0 torch==2.5.1
else
    source "$VENV_DIR/bin/activate"
    echo "‚úì Activated venv: $VENV_DIR"
fi

# Verify
if ! python -c "import vllm" 2>/dev/null; then
    echo -e "${RED}ERROR: vLLM not found!${NC}"
    exit 1
fi

VLLM_VERSION=$(python -c "import vllm; print(vllm.__version__)" 2>/dev/null)
echo "‚úì vLLM version: $VLLM_VERSION"


# Step 3: Launch server
echo -e "\n${YELLOW}[3/3] Launching vLLM server...${NC}"
echo -e "${GREEN}============================================${NC}"
echo "Model: ${MODEL_NAME} (135M params)"
echo "GPU: ${GPU_ID}"
echo "Port: ${PORT}"
echo "Memory Utilization: ${GPU_MEMORY_UTIL} (~300MB only)"
echo "Expected Latency: < 10ms per request"
echo -e "${GREEN}============================================${NC}"
echo ""
echo "üöÄ ULTRA-FAST MODE: SmolLM2 is 3.7x smaller than Qwen-0.5B"
echo "Server starting... Press Ctrl+C to stop"
echo ""

# Log file
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
LOG_FILE="$SCRIPT_DIR/smollm2_server.log"

# Launch vLLM v·ªõi SmolLM2-135M
# NOTE: Model n√†y kh√¥ng c√≥ s·∫µn AWQ version, nh∆∞ng FP16 ƒë√£ ƒë·ªß nhanh!
CUDA_VISIBLE_DEVICES=$GPU_ID python -m vllm.entrypoints.openai.api_server \
    --model "$MODEL_NAME" \
    --dtype float16 \
    --host 0.0.0.0 \
    --port $PORT \
    --gpu-memory-utilization $GPU_MEMORY_UTIL \
    --max-model-len $MAX_MODEL_LEN \
    --max-num-seqs $MAX_NUM_SEQS \
    --enable-prefix-caching \
    --trust-remote-code \
    --disable-log-requests \
    2>&1 | tee "$LOG_FILE"


# ALTERNATIVE: N·∫øu mu·ªën quantize th·ªß c√¥ng ƒë·ªÉ nhanh h∆°n n·ªØa
# Uncomment d√≤ng d∆∞·ªõi v√† comment block tr√™n
# CUDA_VISIBLE_DEVICES=$GPU_ID python -m vllm.entrypoints.openai.api_server \
#     --model "prithivMLmods/SmolLM2-135M-GGUF" \
#     --quantization gguf \
#     --dtype auto \
#     --host 0.0.0.0 \
#     --port $PORT \
#     --gpu-memory-utilization $GPU_MEMORY_UTIL \
#     --max-model-len $MAX_MODEL_LEN \
#     --max-num-seqs $MAX_NUM_SEQS \
#     --enable-prefix-caching \
#     --trust-remote-code \
#     2>&1 | tee "$LOG_FILE"
